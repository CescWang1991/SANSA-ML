% easychair.tex,v 3.2 2012/05/15
%
% Select appropriate paper format in your document class as
% instructed by your conference organizers. Only withtimes
% and notimes can be used in proceedings created by EasyChair
%
% The available formats are 'letterpaper' and 'a4paper' with
% the former being the default if omitted as in the example
% below.
%
\documentclass{easychair}
%\documentclass[debug]{easychair}
%\documentclass[verbose]{easychair}
%\documentclass[notimes]{easychair}
%\documentclass[withtimes]{easychair}
%\documentclass[a4paper]{easychair}
%\documentclass[letterpaper]{easychair}

% This provides the \BibTeX macro
\usepackage{doc}
% \usepackage{makeidx}

% In order to save space or manage large tables or figures in a
% landcape-like text, you can use the rotating and pdflscape
% packages. Uncomment the desired from the below.
%
% \usepackage{rotating}
% \usepackage{pdflscape}

% If you plan on including some algorithm specification, we recommend
% the below package. Read more details on the custom options of the
% package documentation.
%
%\usepackage{algorithm2e}
\usepackage[ruled,vlined]{algorithm2e}
%\newcommand{\algorithmstyle}[1]{\renewcommand{\algocf@style}{#1}}
\DontPrintSemicolon


%\makeindex

%% Document
%%
\begin{document}

%% Front Matter
%%
% Regular title as in the article class.
%
\title{Efficient Graph Kernels for RDF data using Spark}

% \titlerunning{} has to be set to either the main title or its shorter
% version for the running heads. When processed by
% EasyChair, this command is mandatory: a document without \titlerunning
% will be rejected by EasyChair

\titlerunning{Efficient Graph Kernels for RDF data using Spark}

% Authors are joined by \and. Their affiliations are given by \inst, which indexes
% into the list defined using \institute
%
\author{
Bernhard Japes\inst{1}
\and
Shinho Kang\inst{2}
}

% Institutes for affiliations are also joined by \and,
\institute{
  Informatik III, Universit\"at Bonn,
  Germany\\
  \email{bernhard.japes@uni-bonn.de}
\and
   Informatik III, Universit\"at Bonn,
   Germany\\
   \email{TODO}\\
 }
%  \authorrunning{} has to be set for the shorter version of the authors' names;
% otherwise a warning will be rendered in the running heads. When processed by
% EasyChair, this command is mandatory: a document without \authorrunning
% will be rejected by EasyChair

\authorrunning{Bernhard and Shinho}


\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
In this paper we study the application of graph kernels for RDF data using the popular Apache Spark\footnote{http://spark.apache.org} engine in combination with the SANSA-Stack\footnote{http://www.sansa-stack.net} data flow utilities.
We focus on an implementation of the Intersection Tree Path (ITP) Kernel, published by Gerben Klaas Dirk de Vries and Steven de Rooij in \cite{FGK}, that is based on the concept of constructing a tree for each instance and counting the number of paths in that tree.

TODO: Add further information about implementation and/or results

\end{abstract}

\setcounter{tocdepth}{2}
\pagestyle{empty}

%------------------------------------------------------------------------------
\section{Introduction}
\label{sect:Introduction}
The increasing availability of structured data and the rise of the semantic web pose new challenges for machine learning and data mining. As an official standard, the \textit{Resource Description Framework} (RDF) is commonly used to represent those graphs, which led to research on how to use the RDF structure to predict links and labels of instances efficiently. Most of the current approaches to mining structured graph-data focus on specific semantic properties and are individually designed for different problems \cite{Rettinger2009, Huang2011}.

Kernels, however, have already been proven to be useful as a much more flexible approach for Pattern Analysis in different areas \cite{Shawe-Taylor2004}, which resulted in further research on specific graph kernels for RDF. The main drawback of most of these graph kernels, including the state-of-the-art \textit{Weisfeiler-Lehman} (WL) RDF kernel, is their computation time as shown in \cite{deVries2013}. In \cite{FGK} Gerben Klaas Dirk de Vries and Steven de Rooij present a \textit{Fast and Simple Graph Kernel for RDF} with just a slightly worse prediction performance than the WL graph kernel, but the huge upside of being 10 times faster in practice. Their idea of a fast and simple, but scalable kernel also seems to be promising for big data applications. However several adaptions of their algorithm are required to ensure consistent computations on distributed data sets using the Apache Spark engine.

%------------------------------------------------------------------------------
\section{Approach}
\label{sect:Approach}

The graph kernel presented in \cite{FGK} is based on the idea that instances are represented by their subgraphs. This assumption implies that we should be able to explicitly compute a feature vector for each instance by constructing a tree starting from the instance vertex, up to a certain depth $d$ and counting the paths. Now taking the dot product of two feature vectors is essentially the intersection of both trees. The original pseudo-code for the so called Intersection Tree Path Kernel (ITP) is given in Algorithm \ref{alg:ITP}.

\begin{algorithm}
 \label{alg:ITP}
 \KwData{a set of RDF triples $R$, a set of instances $I$ and a max depth $d_{max}$ }
 \KwResult{a set of feature vectors F corresponding to the instances I}
 \textbf{Comment:} $pathMap$ is a global map between paths of vertices and edges and integers\;
 \;
 - set $pathIdx = 1$\;
 - for each $i \in I$\; \Indp
 - create a new feature vector $fv$\;
 - do $processVertex(i,i,[],fv,d_{max})$\;
 - add $fv$ to $F$\; \Indm
 \;
 \textbf{function} $processVertex(v,root,path,fv,d)$\; \Indp
 - if $d = 0$, return \;
 - for each $(v,p,o) \in R$\; \Indp
 - if $o$ is $root$, set $path = [path,p,rootID]$\;
 - else, $path = [path, p, o]$\;
 - if $pathMap(path)$ is undifned\; \Indp
 - set $pathMap(path) = pathIdx$
 - set$pathIdx = pathIdx + 1$\; \Indm
 - set $fv(pathMap(path)) = fv(pathMap(path)) + 1$\;
 - do $processVertex(o,root,path,fv,d-1)$\;
 \;
\caption{The Intersection Tree Path (ITP) Kernel as introduced in \cite{FGK}}
\end{algorithm}

This algorithm can be directly applied to small and medium size datasets. However, big datasets are commonly distributed on different nodes as a \textit{Resilient Distributed Dataset} (RDD), or \textit{DataFrame} (DF) \cite{RDDpaper} and should be processed in parallel by using frameworks like Spark and SANSA. One main aspect of the ITP kernel is the iterative construction of paths and the associated $pathMap$ that assigns a unique integer to each path. To optimize the performance on distributed data we want to avoid this iterative and not parallelized construction if possible.

This can be achieved by using a different representation of the constructed trees as shown in algorithm \ref{alg:FTGK}. To keep track of the different \textit{Uniform Resource Identifiers} (URIs) of subjects and objects during the parallelized construction of trees, we start off by mapping these URIs iteratively to integers. Furthermore we map the instances to their respective label and transform the TripleRDD, generated by using the SANSA RDF utilities, to a DataFrame. Construction of the paths of trees is implemented as a series of SQL queries on DataFrames utilizing the inherent structure. Instead of representing each path as an integer on a $pathMap$ as in the ITP kernel, we construct each path in $pathDF$ as a \textit{String} containing the predicates and objects respectively subjects.

Based on all the paths created and stored in $pathDF$ we can now compute the trees by aggregating paths and collecting them as an Array[\textit{String}] for each subject. Now those Array[\textit{String}] can not only be interpreted as a tree in form of a list of paths, but also as a regular text document using paths as a vocabulary.  In doing so we can make use of the \textit{Spark ML CountVectorizer} that is designed to convert a collection of text documents to vectors of token counts, extracting the vocabulary in form of a sparse representation. This basically replaces the $pathMap$ with the huge upside of performing parallelized on distributed data. 

The Array[\textit{String}] of each subject is transformed to a sparse vector containing information which paths are part of the tree with a consistent mapping for all subjects covering all the occurring paths. These sparse vectors can be used as feature vectors for different machine learning algorithms. 

\begin{algorithm}
 \label{alg:FTGK}
 \KwData{a TripleRDD $R$, an instance DF $I$ and a max depth $d_{max}$ }
 \KwResult{a set of feature vectors F corresponding to the instances I}
 \;
 \textbf{Initialization:}\; \Indp
 - map subjects and objects in $R$ and $I$ to unique integers\;
 - map instances in $I$ to their label\;
 - transform $R$ into a DF $T$ with columns ($s,p,o$) for subject, predicate and object\; \Indm
\; 
\textbf{Construct the paths of trees using SQL-Queries:}\; \Indp
- create a new $pathDF$ with columns ($s,path,o$), where $path$ is the concatenation of $p$ and $o$ based on $T$\;
- for depth $d$ in $[2,d_{max}]$:\; \Indp
- create an empty $DF_d$ with columns ($s,path,o$)\;
- find rows $r_s = (s_s,path_s,o_s)$ in the $pathDF$ where $o_s$ is the subject $s_o$ of another row $r_o=(s_o,path_o,o_o)$\;
- add these rows to $DF_d$ as ($s,path,o$) = $(s_s,path_s+path_o,o_o)$\; \Indm
- add all the $DF_d$ to the $pathDF$\; \Indm
 \;
 \textbf{Construct the feature vectors:}\; \Indp
 - drop the column $o$\;
 - aggregate the rows of $pathDF$ for each $s$ and collect all the $path$ as a list in a new column $paths$ of type Array[\textit{String}]\;
 - use the \textit{Spark ML CountVectorizer} to transform this set of Array[\textit{String}] to sparse feature vectors per subject\;
 \;
\caption{The RDFFastTreeGraphKernel}
\end{algorithm}

%------------------------------------------------------------------------------
\section{Implementation}
\label{sect:Implementation}


%------------------------------------------------------------------------------
\section{Evaluation}
\label{sect:Evaluation}


%------------------------------------------------------------------------------
\section{Conclusion}
\label{sect:Conclusion}

\subsection{Project timeline}


\subsection{Further ideas}


%------------------------------------------------------------------------------


%------------------------------------------------------------------------------
% Refs:
%
\label{sect:bib}
\bibliographystyle{plain}
%\bibliographystyle{alpha}
%\bibliographystyle{unsrt}
%\bibliographystyle{abbrv}
\bibliography{fast_graph_kernel}

%------------------------------------------------------------------------------
% Index
%\printindex

%------------------------------------------------------------------------------
\end{document}

% EOF
